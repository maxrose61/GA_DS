# Basic Data science imports
%matplotlib inline
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# use requests library to interact with a URL
import requests
from requests.auth import HTTPBasicAuth
url = 'http://www.totaleclips.com/api/v1/assets?vendorid=1525&tms_rootid=12009493'
cred = ('apitester', 'apitester')
r = requests.get(url,auth=cred )
rdf = pd.DataFrame(r.json())

# sklearn
linreg = LinearRegression()
linreg.fit(X, y)
print linreg.intercept_
print linreg.coef_

## Cross val score
knn = KNeighborsClassifier(n_neighbors=3)
from sklearn.cross_validation import cross_val_score
cross_val_score(knn, X, y, cv=5, scoring='accuracy').mean()


### Standard Scaler
# standardize the features
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X)
X_scaled = scaler.transform(X)

from sklearn import metrics
print 'MAE:', metrics.mean_absolute_error(true, pred)
print 'MSE:', metrics.mean_squared_error(true, pred)
print 'RMSE:', np.sqrt(metrics.mean_squared_error(true, pred))

# train_test_split
from sklearn.cross_validation import train_test_split
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=1)

# define a function that accepts a list of features and returns testing RMSE
def train_test_rmse(feature_cols):
    X = bikes[feature_cols]
    y = bikes.total
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)
    linreg = LinearRegression()
    linreg.fit(X_train, y_train)
    y_pred = linreg.predict(X_test)
    return np.sqrt(metrics.mean_squared_error(y_test, y_pred))


    train_test_split(X, y, random_state=123)

# Seaborn
sns.lmplot(x='temp_F', y='total', data=bikes, aspect=1.5, scatter_kws={'alpha':0.2})
# multiple scatter plots in Seaborn
sns.pairplot(bikes, x_vars=feature_cols, y_vars='total', kind='reg')
# visualize correlation matrix in Seaborn using a heatmap
sns.heatmap(bikes.corr())


# boxplot

# box plot of rentals, grouped by season
bikes.boxplot(column='total', by='season')


## scatter matrix
pd.scatter_matrix(X, c=colors[beer.cluster], figsize=(10,10), s=100)

# RMSE
# split X and y into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)

# create a NumPy array with the same shape as y_test
y_null = np.zeros_like(y_test, dtype=float)

# fill the array with the mean value of y_test
y_null.fill(y_test.mean())
y_null

# compute null RMSE
np.sqrt(metrics.mean_squared_error(y_test, y_null))

## NLP
import pandas as pd
import numpy as np
import scipy as sp
from sklearn.cross_validation import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from textblob import TextBlob, Word
from nltk.stem.snowball import SnowballStemmer
%matplotlib inline

NULL ACCURACY 13_natural_language_processing cell 28

TfidfVectorizer
USE fit_transform after define X, and again on Xtrain

TextBlob:
	sentiment
	word/sentence split
	spell correct


FEATURES = columns with usable data i.e. numeric?

PCA Dimensionality reduction
%matplotlib inline
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

from sklearn import decomposition ###
from sklearn.cross_validation import cross_val_score

iris['species_num'] = iris.species.map({'Iris-setosa':0, 'Iris-versicolor':1, 'Iris-virginica':2})

Decision Trees

PROJECT compute null accuracies!!!

Run cross validation in end to validate model.

hitters.dropna(inplace=True)

### Time Series
data2 = pd.read_csv('../data/rossmann.csv', skipinitialspace=True, low_memory=False,parse_dates=['Date'])
Resample == groupby for dates
data['Sales'].resample('D').mean().autocorr(lag=14)


Time Series modeling
no standard cross validation

detrending - remove up or down trend. Standard Scaler
simialr to error in linear regression
diff

ARIMA
AR auto regression
AR(p) - lag
MA moving avg model
takes previous error for prediction

ARMA combine autoregressive and moving average

ARIMA auto regressibe integrated moving average model
predict the difference of the model
ARIMA(p,d,q)
ARIMA not relying on stationary data.

PLOT AUTOCORR
%matplotlib inline
from pandas.tools.plotting import autocorrelation_plot
autocorrelation_plot(store1_data.Sales)
STATSMODELS
from statsmodels.graphics.tsaplots import plot_acf
plot_acf(store1_data.Sales, lags=10)
## allows fixed number of lag values

from statsmodels.tsa.arima_model import ARIMA


SCATTER CHART
enr_complete.plot(kind='scatter',x='OverdueActivities',y='gradeID')

BAR CHARTS for quick Vis of individual Feature
parcel.StreetSurface.value_counts().plot(kind='bar')

